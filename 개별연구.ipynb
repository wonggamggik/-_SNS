{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61LhQwcmX2nJ",
        "outputId": "73da3767-0721-4c76-d53e-d00fc5573584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 65.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n",
            "--2022-06-11 20:59:29--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22e9:9f55, 2406:da00:ff00::22c5:2ef4, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=YrEUVdLMu17iIy5X3pbp3OyJ4HA%3D&Expires=1654982969&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2022-06-11 20:59:29--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=YrEUVdLMu17iIy5X3pbp3OyJ4HA%3D&Expires=1654982969&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.197.9\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.197.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  7.40MB/s    in 0.2s    \n",
            "\n",
            "2022-06-11 20:59:30 (7.40 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "--2022-06-11 20:59:35--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22cd:e0db, 2406:da00:ff00::22e9:9f55, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=b2tFMMPYgcmE8M5wZIBIWAAuiao%3D&Expires=1654982975&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2022-06-11 20:59:35--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=b2tFMMPYgcmE8M5wZIBIWAAuiao%3D&Expires=1654982975&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.140.137\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.140.137|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  58.6MB/s    in 0.8s    \n",
            "\n",
            "2022-06-11 20:59:36 (58.6 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# https://wikidocs.net/44249 참조\n",
        "# https://projectlog-eraser.tistory.com/m/21 참조\n",
        "\n",
        "! pip3 install konlpy\n",
        "! wget https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
        "! tar xvfz mecab-0.996-ko-0.9.2.tar.gz > /dev/null 2>&1\n",
        "! ./configure > /dev/null 2>&1\n",
        "! make > /dev/null 2>&1\n",
        "! make check > /dev/null 2>&1\n",
        "! make install > /dev/null 2>&1\n",
        "! ldconfig > /dev/null 2>&1\n",
        "! wget https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
        "! tar xvfz mecab-ko-dic-2.1.1-20180720.tar.gz > /dev/null 2>&1\n",
        "! ./configure > /dev/null 2>&1\n",
        "! make > /dev/null 2>&1\n",
        "! make install > /dev/null 2>&1\n",
        "! apt-get update > /dev/null 2>&1\n",
        "! apt-get upgrade > /dev/null 2>&1\n",
        "! apt install curl > /dev/null 2>&1\n",
        "! apt install git > /dev/null 2>&1\n",
        "! bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)  > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQfCw47OjA1r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "import tensorflow as tf\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaG-FR6ajxBq",
        "outputId": "923140d8-b062-4d71-a2f8-92dabf5c0f4c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ratings_test.txt', <http.client.HTTPMessage at 0x7f430d2d7650>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/wonggamggik/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/wonggamggik/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "JIgr63oPxXSF",
        "outputId": "0dc54dec-906b-45e8-9335-2de96773505d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f430b3d2450>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "train_data = pd.read_table('ratings_train.txt')\n",
        "test_data = pd.read_table('ratings_test.txt')\n",
        "\n",
        "# document 열과 label 열의 중복을 제외한 값의 개수\n",
        "train_data['document'].nunique(), train_data['label'].nunique()\n",
        "\n",
        "# document 열의 중복 제거\n",
        "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
        "\n",
        "train_data['label'].value_counts().plot(kind = 'bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U5pC_8PjxhZ",
        "outputId": "e68261e2-24cc-404e-8c02-ec92ca7eb97e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HsCMOLiyYbP",
        "outputId": "7fabe6a0-af81-4593-d28b-0aae6066525e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# 한글과 공백을 제외하고 모두 제거\n",
        "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxsJS5ukyY-U"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.dropna(how = 'any') # Null 샘플들은 레이블이 긍정일 수도 있고, 부정일 수도 있습니다. 아무런 의미도 없는 데이터므로 제거해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTv_zQOQyhYd",
        "outputId": "ecd1b7ea-019b-4859-fcd6-6e45a9142397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 후 테스트용 샘플의 개수 : 54644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ],
      "source": [
        "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
        "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
        "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
        "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "test_data = test_data.dropna(how='any') # Null 값 제거\n",
        "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNJLHXeIyzJJ"
      },
      "outputs": [],
      "source": [
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQnT8aqtyzmW"
      },
      "outputs": [],
      "source": [
        "okt = Okt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ28uUNRyzop",
        "outputId": "4270d416-14f0-4b86-cde9-8a0a095e489c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 154288/154288 [08:50<00:00, 290.77it/s]\n"
          ]
        }
      ],
      "source": [
        "X_train = []\n",
        "for sentence in tqdm(train_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    X_train.append(stopwords_removed_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djOLz2oGyzq1",
        "outputId": "b92d6fd2-8d6b-4a58-aa3c-8ddb5150e412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54644/54644 [03:15<00:00, 280.21it/s]\n"
          ]
        }
      ],
      "source": [
        "X_test = []\n",
        "for sentence in tqdm(test_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    X_test.append(stopwords_removed_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyC-SOByy-h0"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi04hBobzB4I"
      },
      "outputs": [],
      "source": [
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykakybY3zB6u",
        "outputId": "ebc193d8-af0b-4f64-f4db-36890b923922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합의 크기 : 20395\n"
          ]
        }
      ],
      "source": [
        "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
        "# 0번 패딩 토큰을 고려하여 + 1\n",
        "vocab_size = total_cnt - rare_cnt + 1\n",
        "print('단어 집합의 크기 :',vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGz2riuizB85"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(vocab_size) \n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOGLH7pczB_r"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(train_data['label'])\n",
        "y_test = np.array(test_data['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fogy23VjzCCJ"
      },
      "outputs": [],
      "source": [
        "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLtc5AnjzXh_",
        "outputId": "1efbca1a-afed-4047-a0fb-6294aa415f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "153268\n",
            "153268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = asarray(arr)\n"
          ]
        }
      ],
      "source": [
        "# 빈 샘플들을 제거\n",
        "X_train = np.delete(X_train, drop_train, axis=0)\n",
        "y_train = np.delete(y_train, drop_train, axis=0)\n",
        "print(len(X_train))\n",
        "print(len(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "HYmiH-jFzXkZ",
        "outputId": "367f2d0d-ad0e-41a9-9d77-73862f006903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "리뷰의 최대 길이 : 69\n",
            "리뷰의 평균 길이 : 10.615705822480884\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "print('리뷰의 최대 길이 :',max(len(review) for review in X_train))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
        "plt.hist([len(review) for review in X_train], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIafZ0PFzXm0"
      },
      "outputs": [],
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "  count = 0\n",
        "  for sentence in nested_list:\n",
        "    if(len(sentence) <= max_len):\n",
        "        count = count + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xex1StHazXpY",
        "outputId": "d5b0530c-ec8c-4d30-d5ed-7301d9b3e190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 중 길이가 30 이하인 샘플의 비율: 94.61009473601796\n"
          ]
        }
      ],
      "source": [
        "max_len = 30\n",
        "below_threshold_len(max_len, X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2v39Y8Q5eR9"
      },
      "outputs": [],
      "source": [
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9Hktxtv5eT5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e4299f-e943-4926-c8ca-e353741532b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.4019 - acc: 0.8180\n",
            "Epoch 1: val_acc improved from -inf to 0.80838, saving model to best_model.h5\n",
            "479/479 [==============================] - 13s 10ms/step - loss: 0.4016 - acc: 0.8182 - val_loss: 0.4167 - val_acc: 0.8084\n",
            "Epoch 2/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.3317 - acc: 0.8568\n",
            "Epoch 2: val_acc did not improve from 0.80838\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.3316 - acc: 0.8569 - val_loss: 0.4189 - val_acc: 0.8048\n",
            "Epoch 3/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.3039 - acc: 0.8701\n",
            "Epoch 3: val_acc improved from 0.80838 to 0.81389, saving model to best_model.h5\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.3039 - acc: 0.8702 - val_loss: 0.4037 - val_acc: 0.8139\n",
            "Epoch 4/250\n",
            "472/479 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.8813\n",
            "Epoch 4: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.2824 - acc: 0.8811 - val_loss: 0.4095 - val_acc: 0.8079\n",
            "Epoch 5/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.8901\n",
            "Epoch 5: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.2644 - acc: 0.8901 - val_loss: 0.4102 - val_acc: 0.8113\n",
            "Epoch 6/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.8987\n",
            "Epoch 6: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.2474 - acc: 0.8989 - val_loss: 0.4246 - val_acc: 0.8095\n",
            "Epoch 7/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9062\n",
            "Epoch 7: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.2318 - acc: 0.9062 - val_loss: 0.4590 - val_acc: 0.7746\n",
            "Epoch 8/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9145\n",
            "Epoch 8: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.2164 - acc: 0.9145 - val_loss: 0.4813 - val_acc: 0.7783\n",
            "Epoch 9/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9210\n",
            "Epoch 9: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.2012 - acc: 0.9210 - val_loss: 0.5068 - val_acc: 0.7625\n",
            "Epoch 10/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9275\n",
            "Epoch 10: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.1875 - acc: 0.9275 - val_loss: 0.4997 - val_acc: 0.7808\n",
            "Epoch 11/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.1742 - acc: 0.9327\n",
            "Epoch 11: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.1742 - acc: 0.9327 - val_loss: 0.5659 - val_acc: 0.7503\n",
            "Epoch 12/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.1621 - acc: 0.9381\n",
            "Epoch 12: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.1624 - acc: 0.9379 - val_loss: 0.6306 - val_acc: 0.7325\n",
            "Epoch 13/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9430\n",
            "Epoch 13: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.1506 - acc: 0.9430 - val_loss: 0.6658 - val_acc: 0.7311\n",
            "Epoch 14/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9475\n",
            "Epoch 14: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 3s 7ms/step - loss: 0.1397 - acc: 0.9476 - val_loss: 0.6251 - val_acc: 0.7549\n",
            "Epoch 15/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9513\n",
            "Epoch 15: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.1296 - acc: 0.9513 - val_loss: 0.6620 - val_acc: 0.7539\n",
            "Epoch 16/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9556\n",
            "Epoch 16: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.1197 - acc: 0.9555 - val_loss: 0.7531 - val_acc: 0.7265\n",
            "Epoch 17/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9592\n",
            "Epoch 17: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.1110 - acc: 0.9592 - val_loss: 0.8343 - val_acc: 0.7165\n",
            "Epoch 18/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9625\n",
            "Epoch 18: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.1026 - acc: 0.9624 - val_loss: 0.8138 - val_acc: 0.7393\n",
            "Epoch 19/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9652\n",
            "Epoch 19: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0950 - acc: 0.9652 - val_loss: 0.8433 - val_acc: 0.7372\n",
            "Epoch 20/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9680\n",
            "Epoch 20: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0879 - acc: 0.9680 - val_loss: 0.8412 - val_acc: 0.7526\n",
            "Epoch 21/250\n",
            "472/479 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9706\n",
            "Epoch 21: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0812 - acc: 0.9705 - val_loss: 0.9567 - val_acc: 0.7306\n",
            "Epoch 22/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9729\n",
            "Epoch 22: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0753 - acc: 0.9729 - val_loss: 1.0489 - val_acc: 0.7300\n",
            "Epoch 23/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9749\n",
            "Epoch 23: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0691 - acc: 0.9749 - val_loss: 1.0017 - val_acc: 0.7415\n",
            "Epoch 24/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9769\n",
            "Epoch 24: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0644 - acc: 0.9769 - val_loss: 1.0791 - val_acc: 0.7292\n",
            "Epoch 25/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9791\n",
            "Epoch 25: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0592 - acc: 0.9791 - val_loss: 1.1784 - val_acc: 0.7238\n",
            "Epoch 26/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9800\n",
            "Epoch 26: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0547 - acc: 0.9800 - val_loss: 1.1418 - val_acc: 0.7368\n",
            "Epoch 27/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9819\n",
            "Epoch 27: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0506 - acc: 0.9819 - val_loss: 1.2045 - val_acc: 0.7321\n",
            "Epoch 28/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9838\n",
            "Epoch 28: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0467 - acc: 0.9837 - val_loss: 1.3095 - val_acc: 0.7252\n",
            "Epoch 29/250\n",
            "472/479 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9850\n",
            "Epoch 29: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0435 - acc: 0.9849 - val_loss: 1.3203 - val_acc: 0.7236\n",
            "Epoch 30/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0401 - acc: 0.9861\n",
            "Epoch 30: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0401 - acc: 0.9861 - val_loss: 1.2667 - val_acc: 0.7334\n",
            "Epoch 31/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9871\n",
            "Epoch 31: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0374 - acc: 0.9871 - val_loss: 1.4126 - val_acc: 0.7242\n",
            "Epoch 32/250\n",
            "472/479 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9882\n",
            "Epoch 32: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0347 - acc: 0.9883 - val_loss: 1.4502 - val_acc: 0.7286\n",
            "Epoch 33/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9887\n",
            "Epoch 33: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0327 - acc: 0.9887 - val_loss: 1.5044 - val_acc: 0.7236\n",
            "Epoch 34/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9895\n",
            "Epoch 34: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0311 - acc: 0.9896 - val_loss: 1.5333 - val_acc: 0.7227\n",
            "Epoch 35/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9902\n",
            "Epoch 35: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0291 - acc: 0.9902 - val_loss: 1.5205 - val_acc: 0.7311\n",
            "Epoch 36/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9908\n",
            "Epoch 36: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0267 - acc: 0.9908 - val_loss: 1.6846 - val_acc: 0.7179\n",
            "Epoch 37/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9913\n",
            "Epoch 37: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0259 - acc: 0.9913 - val_loss: 1.6277 - val_acc: 0.7288\n",
            "Epoch 38/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9917\n",
            "Epoch 38: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0244 - acc: 0.9917 - val_loss: 1.6538 - val_acc: 0.7256\n",
            "Epoch 39/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9920\n",
            "Epoch 39: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0232 - acc: 0.9920 - val_loss: 1.6430 - val_acc: 0.7319\n",
            "Epoch 40/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9929\n",
            "Epoch 40: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0219 - acc: 0.9928 - val_loss: 1.7855 - val_acc: 0.7196\n",
            "Epoch 41/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9930\n",
            "Epoch 41: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0212 - acc: 0.9930 - val_loss: 1.6562 - val_acc: 0.7322\n",
            "Epoch 42/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9934\n",
            "Epoch 42: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0198 - acc: 0.9933 - val_loss: 1.7490 - val_acc: 0.7288\n",
            "Epoch 43/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9936\n",
            "Epoch 43: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0186 - acc: 0.9936 - val_loss: 1.7861 - val_acc: 0.7303\n",
            "Epoch 44/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9938\n",
            "Epoch 44: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0181 - acc: 0.9939 - val_loss: 1.7978 - val_acc: 0.7306\n",
            "Epoch 45/250\n",
            "472/479 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9940\n",
            "Epoch 45: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0177 - acc: 0.9940 - val_loss: 1.7259 - val_acc: 0.7366\n",
            "Epoch 46/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9943\n",
            "Epoch 46: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0165 - acc: 0.9943 - val_loss: 1.7517 - val_acc: 0.7320\n",
            "Epoch 47/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9946\n",
            "Epoch 47: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0160 - acc: 0.9946 - val_loss: 1.8780 - val_acc: 0.7186\n",
            "Epoch 48/250\n",
            "472/479 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9946\n",
            "Epoch 48: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0157 - acc: 0.9946 - val_loss: 1.9560 - val_acc: 0.7264\n",
            "Epoch 49/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9948\n",
            "Epoch 49: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0151 - acc: 0.9948 - val_loss: 1.9346 - val_acc: 0.7281\n",
            "Epoch 50/250\n",
            "472/479 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9949\n",
            "Epoch 50: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0146 - acc: 0.9949 - val_loss: 1.9510 - val_acc: 0.7307\n",
            "Epoch 51/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9953\n",
            "Epoch 51: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0139 - acc: 0.9953 - val_loss: 1.9463 - val_acc: 0.7282\n",
            "Epoch 52/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9951\n",
            "Epoch 52: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0140 - acc: 0.9951 - val_loss: 1.8883 - val_acc: 0.7287\n",
            "Epoch 53/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9953\n",
            "Epoch 53: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0130 - acc: 0.9953 - val_loss: 2.0278 - val_acc: 0.7261\n",
            "Epoch 54/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9955\n",
            "Epoch 54: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0129 - acc: 0.9955 - val_loss: 1.8906 - val_acc: 0.7370\n",
            "Epoch 55/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9956\n",
            "Epoch 55: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0129 - acc: 0.9956 - val_loss: 1.9920 - val_acc: 0.7244\n",
            "Epoch 56/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9958\n",
            "Epoch 56: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0125 - acc: 0.9958 - val_loss: 1.9283 - val_acc: 0.7238\n",
            "Epoch 57/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0118 - acc: 0.9958\n",
            "Epoch 57: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0118 - acc: 0.9958 - val_loss: 1.9644 - val_acc: 0.7263\n",
            "Epoch 58/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9958\n",
            "Epoch 58: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0119 - acc: 0.9958 - val_loss: 1.9714 - val_acc: 0.7263\n",
            "Epoch 59/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9959\n",
            "Epoch 59: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0116 - acc: 0.9959 - val_loss: 2.1369 - val_acc: 0.7170\n",
            "Epoch 60/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9959\n",
            "Epoch 60: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0114 - acc: 0.9959 - val_loss: 2.0083 - val_acc: 0.7257\n",
            "Epoch 61/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9960\n",
            "Epoch 61: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0115 - acc: 0.9960 - val_loss: 1.8799 - val_acc: 0.7363\n",
            "Epoch 62/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9961\n",
            "Epoch 62: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0114 - acc: 0.9961 - val_loss: 1.9507 - val_acc: 0.7302\n",
            "Epoch 63/250\n",
            "472/479 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9962\n",
            "Epoch 63: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0109 - acc: 0.9962 - val_loss: 1.9513 - val_acc: 0.7297\n",
            "Epoch 64/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9963\n",
            "Epoch 64: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0103 - acc: 0.9963 - val_loss: 1.9764 - val_acc: 0.7351\n",
            "Epoch 65/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9962\n",
            "Epoch 65: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0107 - acc: 0.9962 - val_loss: 2.0741 - val_acc: 0.7236\n",
            "Epoch 66/250\n",
            "472/479 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9962\n",
            "Epoch 66: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0105 - acc: 0.9962 - val_loss: 2.0286 - val_acc: 0.7320\n",
            "Epoch 67/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9964\n",
            "Epoch 67: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0106 - acc: 0.9964 - val_loss: 1.9081 - val_acc: 0.7387\n",
            "Epoch 68/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0101 - acc: 0.9963\n",
            "Epoch 68: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0101 - acc: 0.9963 - val_loss: 2.0318 - val_acc: 0.7334\n",
            "Epoch 69/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9965\n",
            "Epoch 69: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0098 - acc: 0.9965 - val_loss: 1.9588 - val_acc: 0.7405\n",
            "Epoch 70/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9962\n",
            "Epoch 70: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0105 - acc: 0.9962 - val_loss: 1.9481 - val_acc: 0.7406\n",
            "Epoch 71/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9963\n",
            "Epoch 71: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0099 - acc: 0.9963 - val_loss: 1.9581 - val_acc: 0.7362\n",
            "Epoch 72/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9966\n",
            "Epoch 72: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0096 - acc: 0.9966 - val_loss: 2.1033 - val_acc: 0.7261\n",
            "Epoch 73/250\n",
            "472/479 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9964\n",
            "Epoch 73: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0100 - acc: 0.9964 - val_loss: 1.9161 - val_acc: 0.7397\n",
            "Epoch 74/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9966\n",
            "Epoch 74: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0094 - acc: 0.9966 - val_loss: 2.0765 - val_acc: 0.7381\n",
            "Epoch 75/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0094 - acc: 0.9966\n",
            "Epoch 75: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0094 - acc: 0.9966 - val_loss: 1.9708 - val_acc: 0.7381\n",
            "Epoch 76/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0096 - acc: 0.9965\n",
            "Epoch 76: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0096 - acc: 0.9965 - val_loss: 2.1083 - val_acc: 0.7286\n",
            "Epoch 77/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9966\n",
            "Epoch 77: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0093 - acc: 0.9965 - val_loss: 1.9789 - val_acc: 0.7419\n",
            "Epoch 78/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9966\n",
            "Epoch 78: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0094 - acc: 0.9966 - val_loss: 1.9814 - val_acc: 0.7346\n",
            "Epoch 79/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9965\n",
            "Epoch 79: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0095 - acc: 0.9964 - val_loss: 1.9712 - val_acc: 0.7326\n",
            "Epoch 80/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9966\n",
            "Epoch 80: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0091 - acc: 0.9966 - val_loss: 1.9965 - val_acc: 0.7336\n",
            "Epoch 81/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9966\n",
            "Epoch 81: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0093 - acc: 0.9966 - val_loss: 1.9364 - val_acc: 0.7409\n",
            "Epoch 82/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9966\n",
            "Epoch 82: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0091 - acc: 0.9966 - val_loss: 2.0400 - val_acc: 0.7382\n",
            "Epoch 83/250\n",
            "472/479 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9966\n",
            "Epoch 83: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0091 - acc: 0.9966 - val_loss: 2.0220 - val_acc: 0.7372\n",
            "Epoch 84/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9966\n",
            "Epoch 84: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0091 - acc: 0.9966 - val_loss: 2.0033 - val_acc: 0.7350\n",
            "Epoch 85/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9966\n",
            "Epoch 85: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0092 - acc: 0.9966 - val_loss: 1.9491 - val_acc: 0.7354\n",
            "Epoch 86/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9965\n",
            "Epoch 86: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0092 - acc: 0.9965 - val_loss: 2.0507 - val_acc: 0.7297\n",
            "Epoch 87/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9967\n",
            "Epoch 87: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0089 - acc: 0.9967 - val_loss: 1.9864 - val_acc: 0.7429\n",
            "Epoch 88/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9966\n",
            "Epoch 88: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0090 - acc: 0.9966 - val_loss: 2.0632 - val_acc: 0.7255\n",
            "Epoch 89/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9966\n",
            "Epoch 89: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0088 - acc: 0.9966 - val_loss: 2.0323 - val_acc: 0.7407\n",
            "Epoch 90/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0090 - acc: 0.9965\n",
            "Epoch 90: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0090 - acc: 0.9965 - val_loss: 1.9487 - val_acc: 0.7420\n",
            "Epoch 91/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9967\n",
            "Epoch 91: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0087 - acc: 0.9967 - val_loss: 1.9804 - val_acc: 0.7422\n",
            "Epoch 92/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9966\n",
            "Epoch 92: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0087 - acc: 0.9966 - val_loss: 1.9318 - val_acc: 0.7409\n",
            "Epoch 93/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9966\n",
            "Epoch 93: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0088 - acc: 0.9966 - val_loss: 1.9737 - val_acc: 0.7407\n",
            "Epoch 94/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9967\n",
            "Epoch 94: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0086 - acc: 0.9967 - val_loss: 1.9138 - val_acc: 0.7417\n",
            "Epoch 95/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9967\n",
            "Epoch 95: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0088 - acc: 0.9967 - val_loss: 2.0900 - val_acc: 0.7316\n",
            "Epoch 96/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9966\n",
            "Epoch 96: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0090 - acc: 0.9966 - val_loss: 1.9205 - val_acc: 0.7485\n",
            "Epoch 97/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9967\n",
            "Epoch 97: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0088 - acc: 0.9967 - val_loss: 1.9440 - val_acc: 0.7425\n",
            "Epoch 98/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9966\n",
            "Epoch 98: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0086 - acc: 0.9966 - val_loss: 1.9481 - val_acc: 0.7413\n",
            "Epoch 99/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9967\n",
            "Epoch 99: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0086 - acc: 0.9967 - val_loss: 1.9439 - val_acc: 0.7418\n",
            "Epoch 100/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9967\n",
            "Epoch 100: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0086 - acc: 0.9967 - val_loss: 1.9195 - val_acc: 0.7419\n",
            "Epoch 101/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9966\n",
            "Epoch 101: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0087 - acc: 0.9966 - val_loss: 1.9128 - val_acc: 0.7463\n",
            "Epoch 102/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9966\n",
            "Epoch 102: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0085 - acc: 0.9966 - val_loss: 1.9242 - val_acc: 0.7433\n",
            "Epoch 103/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9967\n",
            "Epoch 103: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0084 - acc: 0.9967 - val_loss: 1.9308 - val_acc: 0.7423\n",
            "Epoch 104/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9967\n",
            "Epoch 104: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 7ms/step - loss: 0.0086 - acc: 0.9967 - val_loss: 1.8198 - val_acc: 0.7499\n",
            "Epoch 105/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0084 - acc: 0.9967\n",
            "Epoch 105: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0084 - acc: 0.9967 - val_loss: 1.9214 - val_acc: 0.7435\n",
            "Epoch 106/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9967\n",
            "Epoch 106: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0085 - acc: 0.9967 - val_loss: 1.8725 - val_acc: 0.7444\n",
            "Epoch 107/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9967\n",
            "Epoch 107: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0083 - acc: 0.9967 - val_loss: 1.9304 - val_acc: 0.7482\n",
            "Epoch 108/250\n",
            "472/479 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9967\n",
            "Epoch 108: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0084 - acc: 0.9967 - val_loss: 1.8980 - val_acc: 0.7446\n",
            "Epoch 109/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9967\n",
            "Epoch 109: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0082 - acc: 0.9967 - val_loss: 1.8544 - val_acc: 0.7488\n",
            "Epoch 110/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0084 - acc: 0.9967\n",
            "Epoch 110: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0084 - acc: 0.9967 - val_loss: 1.8785 - val_acc: 0.7476\n",
            "Epoch 111/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0084 - acc: 0.9966\n",
            "Epoch 111: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0084 - acc: 0.9966 - val_loss: 1.9462 - val_acc: 0.7396\n",
            "Epoch 112/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9967\n",
            "Epoch 112: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0083 - acc: 0.9967 - val_loss: 1.8757 - val_acc: 0.7474\n",
            "Epoch 113/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0081 - acc: 0.9967\n",
            "Epoch 113: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0081 - acc: 0.9967 - val_loss: 1.9279 - val_acc: 0.7469\n",
            "Epoch 114/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0086 - acc: 0.9966\n",
            "Epoch 114: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0086 - acc: 0.9966 - val_loss: 1.9063 - val_acc: 0.7457\n",
            "Epoch 115/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9968\n",
            "Epoch 115: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0085 - acc: 0.9968 - val_loss: 1.9345 - val_acc: 0.7437\n",
            "Epoch 116/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9967\n",
            "Epoch 116: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0081 - acc: 0.9967 - val_loss: 1.9454 - val_acc: 0.7424\n",
            "Epoch 117/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9968\n",
            "Epoch 117: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0082 - acc: 0.9968 - val_loss: 1.9021 - val_acc: 0.7430\n",
            "Epoch 118/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9969\n",
            "Epoch 118: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0083 - acc: 0.9969 - val_loss: 1.8694 - val_acc: 0.7423\n",
            "Epoch 119/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0084 - acc: 0.9967\n",
            "Epoch 119: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0084 - acc: 0.9967 - val_loss: 1.8567 - val_acc: 0.7442\n",
            "Epoch 120/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0082 - acc: 0.9966\n",
            "Epoch 120: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0082 - acc: 0.9966 - val_loss: 1.9071 - val_acc: 0.7457\n",
            "Epoch 121/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9967\n",
            "Epoch 121: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0082 - acc: 0.9967 - val_loss: 1.9133 - val_acc: 0.7454\n",
            "Epoch 122/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9967\n",
            "Epoch 122: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0083 - acc: 0.9967 - val_loss: 1.8980 - val_acc: 0.7484\n",
            "Epoch 123/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9966\n",
            "Epoch 123: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0083 - acc: 0.9966 - val_loss: 1.9559 - val_acc: 0.7431\n",
            "Epoch 124/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0079 - acc: 0.9968\n",
            "Epoch 124: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0079 - acc: 0.9968 - val_loss: 1.9245 - val_acc: 0.7446\n",
            "Epoch 125/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9967\n",
            "Epoch 125: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0080 - acc: 0.9968 - val_loss: 1.9496 - val_acc: 0.7445\n",
            "Epoch 126/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9967\n",
            "Epoch 126: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0082 - acc: 0.9967 - val_loss: 1.9418 - val_acc: 0.7454\n",
            "Epoch 127/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9966\n",
            "Epoch 127: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0083 - acc: 0.9966 - val_loss: 1.9680 - val_acc: 0.7433\n",
            "Epoch 128/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9967\n",
            "Epoch 128: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 5s 10ms/step - loss: 0.0081 - acc: 0.9966 - val_loss: 1.9417 - val_acc: 0.7462\n",
            "Epoch 129/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0082 - acc: 0.9967\n",
            "Epoch 129: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0082 - acc: 0.9967 - val_loss: 1.9487 - val_acc: 0.7363\n",
            "Epoch 130/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9966\n",
            "Epoch 130: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0082 - acc: 0.9966 - val_loss: 1.9414 - val_acc: 0.7446\n",
            "Epoch 131/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9968\n",
            "Epoch 131: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0082 - acc: 0.9968 - val_loss: 1.8468 - val_acc: 0.7487\n",
            "Epoch 132/250\n",
            "472/479 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9967\n",
            "Epoch 132: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0081 - acc: 0.9967 - val_loss: 1.9094 - val_acc: 0.7439\n",
            "Epoch 133/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9967\n",
            "Epoch 133: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0083 - acc: 0.9966 - val_loss: 1.8743 - val_acc: 0.7549\n",
            "Epoch 134/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9967\n",
            "Epoch 134: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0082 - acc: 0.9967 - val_loss: 1.9740 - val_acc: 0.7433\n",
            "Epoch 135/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9966\n",
            "Epoch 135: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0080 - acc: 0.9966 - val_loss: 1.9165 - val_acc: 0.7453\n",
            "Epoch 136/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9967\n",
            "Epoch 136: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0079 - acc: 0.9967 - val_loss: 1.8868 - val_acc: 0.7475\n",
            "Epoch 137/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0082 - acc: 0.9967\n",
            "Epoch 137: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0082 - acc: 0.9967 - val_loss: 1.9930 - val_acc: 0.7376\n",
            "Epoch 138/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9967\n",
            "Epoch 138: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0081 - acc: 0.9967 - val_loss: 1.8602 - val_acc: 0.7502\n",
            "Epoch 139/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9968\n",
            "Epoch 139: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0082 - acc: 0.9968 - val_loss: 1.8581 - val_acc: 0.7396\n",
            "Epoch 140/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9966\n",
            "Epoch 140: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0081 - acc: 0.9966 - val_loss: 1.9047 - val_acc: 0.7403\n",
            "Epoch 141/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9967\n",
            "Epoch 141: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0081 - acc: 0.9967 - val_loss: 1.8738 - val_acc: 0.7481\n",
            "Epoch 142/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9967\n",
            "Epoch 142: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0079 - acc: 0.9967 - val_loss: 1.8546 - val_acc: 0.7492\n",
            "Epoch 143/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9967\n",
            "Epoch 143: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0080 - acc: 0.9968 - val_loss: 1.8911 - val_acc: 0.7456\n",
            "Epoch 144/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9968\n",
            "Epoch 144: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0079 - acc: 0.9968 - val_loss: 1.9317 - val_acc: 0.7465\n",
            "Epoch 145/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9968\n",
            "Epoch 145: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0081 - acc: 0.9968 - val_loss: 1.9233 - val_acc: 0.7425\n",
            "Epoch 146/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9968\n",
            "Epoch 146: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0078 - acc: 0.9968 - val_loss: 1.8790 - val_acc: 0.7454\n",
            "Epoch 147/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9967\n",
            "Epoch 147: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0079 - acc: 0.9967 - val_loss: 1.8954 - val_acc: 0.7495\n",
            "Epoch 148/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9966\n",
            "Epoch 148: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0081 - acc: 0.9966 - val_loss: 1.8545 - val_acc: 0.7486\n",
            "Epoch 149/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9968\n",
            "Epoch 149: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0080 - acc: 0.9968 - val_loss: 1.9218 - val_acc: 0.7391\n",
            "Epoch 150/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0081 - acc: 0.9966\n",
            "Epoch 150: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0081 - acc: 0.9966 - val_loss: 1.8393 - val_acc: 0.7500\n",
            "Epoch 151/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0081 - acc: 0.9967\n",
            "Epoch 151: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0081 - acc: 0.9967 - val_loss: 1.7764 - val_acc: 0.7523\n",
            "Epoch 152/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9965\n",
            "Epoch 152: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0081 - acc: 0.9965 - val_loss: 1.8357 - val_acc: 0.7479\n",
            "Epoch 153/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9968\n",
            "Epoch 153: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0078 - acc: 0.9968 - val_loss: 1.8251 - val_acc: 0.7514\n",
            "Epoch 154/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9967\n",
            "Epoch 154: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0079 - acc: 0.9967 - val_loss: 1.8493 - val_acc: 0.7507\n",
            "Epoch 155/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9968\n",
            "Epoch 155: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0079 - acc: 0.9969 - val_loss: 1.8464 - val_acc: 0.7522\n",
            "Epoch 156/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9968\n",
            "Epoch 156: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0078 - acc: 0.9967 - val_loss: 1.8761 - val_acc: 0.7420\n",
            "Epoch 157/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0080 - acc: 0.9968\n",
            "Epoch 157: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0080 - acc: 0.9968 - val_loss: 1.8212 - val_acc: 0.7537\n",
            "Epoch 158/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9967\n",
            "Epoch 158: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0078 - acc: 0.9967 - val_loss: 1.8830 - val_acc: 0.7474\n",
            "Epoch 159/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9968\n",
            "Epoch 159: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0077 - acc: 0.9968 - val_loss: 1.8355 - val_acc: 0.7429\n",
            "Epoch 160/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9966\n",
            "Epoch 160: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0079 - acc: 0.9966 - val_loss: 1.8691 - val_acc: 0.7459\n",
            "Epoch 161/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9967\n",
            "Epoch 161: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0077 - acc: 0.9967 - val_loss: 1.8878 - val_acc: 0.7454\n",
            "Epoch 162/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9967\n",
            "Epoch 162: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0080 - acc: 0.9967 - val_loss: 1.8264 - val_acc: 0.7531\n",
            "Epoch 163/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9967\n",
            "Epoch 163: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0081 - acc: 0.9967 - val_loss: 1.8067 - val_acc: 0.7418\n",
            "Epoch 164/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9967\n",
            "Epoch 164: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0078 - acc: 0.9967 - val_loss: 1.8290 - val_acc: 0.7507\n",
            "Epoch 165/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9967\n",
            "Epoch 165: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0077 - acc: 0.9967 - val_loss: 1.8083 - val_acc: 0.7538\n",
            "Epoch 166/250\n",
            "472/479 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9966\n",
            "Epoch 166: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0079 - acc: 0.9967 - val_loss: 1.8910 - val_acc: 0.7566\n",
            "Epoch 167/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9968\n",
            "Epoch 167: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0077 - acc: 0.9968 - val_loss: 1.8101 - val_acc: 0.7521\n",
            "Epoch 168/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9968\n",
            "Epoch 168: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0078 - acc: 0.9968 - val_loss: 1.8315 - val_acc: 0.7529\n",
            "Epoch 169/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9965\n",
            "Epoch 169: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0079 - acc: 0.9965 - val_loss: 1.8573 - val_acc: 0.7520\n",
            "Epoch 170/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9966\n",
            "Epoch 170: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0080 - acc: 0.9966 - val_loss: 1.8347 - val_acc: 0.7527\n",
            "Epoch 171/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0079 - acc: 0.9967\n",
            "Epoch 171: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0079 - acc: 0.9967 - val_loss: 1.9271 - val_acc: 0.7494\n",
            "Epoch 172/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9968\n",
            "Epoch 172: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0079 - acc: 0.9968 - val_loss: 1.7633 - val_acc: 0.7606\n",
            "Epoch 173/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9968\n",
            "Epoch 173: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0076 - acc: 0.9968 - val_loss: 1.8747 - val_acc: 0.7439\n",
            "Epoch 174/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9969\n",
            "Epoch 174: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0078 - acc: 0.9968 - val_loss: 1.7830 - val_acc: 0.7523\n",
            "Epoch 175/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9968\n",
            "Epoch 175: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0080 - acc: 0.9968 - val_loss: 1.8643 - val_acc: 0.7468\n",
            "Epoch 176/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9967\n",
            "Epoch 176: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0077 - acc: 0.9967 - val_loss: 1.8106 - val_acc: 0.7503\n",
            "Epoch 177/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9967\n",
            "Epoch 177: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0080 - acc: 0.9967 - val_loss: 1.8043 - val_acc: 0.7503\n",
            "Epoch 178/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9968\n",
            "Epoch 178: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0079 - acc: 0.9968 - val_loss: 1.8197 - val_acc: 0.7545\n",
            "Epoch 179/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9968\n",
            "Epoch 179: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0078 - acc: 0.9968 - val_loss: 1.7876 - val_acc: 0.7569\n",
            "Epoch 180/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0078 - acc: 0.9967\n",
            "Epoch 180: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0078 - acc: 0.9967 - val_loss: 1.8687 - val_acc: 0.7536\n",
            "Epoch 181/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0078 - acc: 0.9969\n",
            "Epoch 181: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0078 - acc: 0.9969 - val_loss: 1.8172 - val_acc: 0.7530\n",
            "Epoch 182/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9968\n",
            "Epoch 182: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0077 - acc: 0.9968 - val_loss: 1.8668 - val_acc: 0.7500\n",
            "Epoch 183/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9968\n",
            "Epoch 183: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0076 - acc: 0.9968 - val_loss: 1.8486 - val_acc: 0.7520\n",
            "Epoch 184/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9968\n",
            "Epoch 184: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0079 - acc: 0.9968 - val_loss: 1.8418 - val_acc: 0.7522\n",
            "Epoch 185/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9966\n",
            "Epoch 185: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0078 - acc: 0.9966 - val_loss: 1.7754 - val_acc: 0.7562\n",
            "Epoch 186/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9967\n",
            "Epoch 186: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0076 - acc: 0.9967 - val_loss: 1.8805 - val_acc: 0.7491\n",
            "Epoch 187/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9967\n",
            "Epoch 187: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0077 - acc: 0.9967 - val_loss: 1.8476 - val_acc: 0.7567\n",
            "Epoch 188/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9969\n",
            "Epoch 188: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0078 - acc: 0.9969 - val_loss: 1.8424 - val_acc: 0.7550\n",
            "Epoch 189/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9968\n",
            "Epoch 189: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0075 - acc: 0.9968 - val_loss: 1.8017 - val_acc: 0.7610\n",
            "Epoch 190/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9968\n",
            "Epoch 190: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0078 - acc: 0.9968 - val_loss: 1.8128 - val_acc: 0.7524\n",
            "Epoch 191/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9967\n",
            "Epoch 191: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0078 - acc: 0.9967 - val_loss: 1.8399 - val_acc: 0.7553\n",
            "Epoch 192/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9969\n",
            "Epoch 192: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0075 - acc: 0.9969 - val_loss: 1.8572 - val_acc: 0.7484\n",
            "Epoch 193/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9967\n",
            "Epoch 193: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0077 - acc: 0.9967 - val_loss: 1.8053 - val_acc: 0.7601\n",
            "Epoch 194/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9967\n",
            "Epoch 194: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0076 - acc: 0.9967 - val_loss: 1.8453 - val_acc: 0.7563\n",
            "Epoch 195/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9967\n",
            "Epoch 195: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0080 - acc: 0.9967 - val_loss: 1.7706 - val_acc: 0.7638\n",
            "Epoch 196/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9967\n",
            "Epoch 196: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0079 - acc: 0.9967 - val_loss: 1.7710 - val_acc: 0.7622\n",
            "Epoch 197/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9967\n",
            "Epoch 197: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0079 - acc: 0.9967 - val_loss: 1.8537 - val_acc: 0.7543\n",
            "Epoch 198/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9968\n",
            "Epoch 198: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0077 - acc: 0.9968 - val_loss: 1.8564 - val_acc: 0.7576\n",
            "Epoch 199/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9967\n",
            "Epoch 199: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0078 - acc: 0.9967 - val_loss: 1.7784 - val_acc: 0.7590\n",
            "Epoch 200/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9968\n",
            "Epoch 200: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0077 - acc: 0.9968 - val_loss: 1.7736 - val_acc: 0.7597\n",
            "Epoch 201/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9967\n",
            "Epoch 201: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0077 - acc: 0.9966 - val_loss: 1.7910 - val_acc: 0.7516\n",
            "Epoch 202/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9968\n",
            "Epoch 202: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0076 - acc: 0.9968 - val_loss: 1.8263 - val_acc: 0.7606\n",
            "Epoch 203/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9965\n",
            "Epoch 203: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0081 - acc: 0.9966 - val_loss: 1.7709 - val_acc: 0.7583\n",
            "Epoch 204/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9967\n",
            "Epoch 204: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0076 - acc: 0.9967 - val_loss: 1.8321 - val_acc: 0.7542\n",
            "Epoch 205/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9968\n",
            "Epoch 205: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0078 - acc: 0.9968 - val_loss: 1.7882 - val_acc: 0.7600\n",
            "Epoch 206/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9967\n",
            "Epoch 206: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0078 - acc: 0.9967 - val_loss: 1.7696 - val_acc: 0.7564\n",
            "Epoch 207/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9967\n",
            "Epoch 207: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0076 - acc: 0.9967 - val_loss: 1.8317 - val_acc: 0.7531\n",
            "Epoch 208/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9968\n",
            "Epoch 208: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0077 - acc: 0.9968 - val_loss: 1.8446 - val_acc: 0.7557\n",
            "Epoch 209/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9966\n",
            "Epoch 209: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0078 - acc: 0.9966 - val_loss: 1.8035 - val_acc: 0.7519\n",
            "Epoch 210/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9968\n",
            "Epoch 210: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0077 - acc: 0.9968 - val_loss: 1.8639 - val_acc: 0.7527\n",
            "Epoch 211/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9967\n",
            "Epoch 211: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0077 - acc: 0.9967 - val_loss: 1.7830 - val_acc: 0.7620\n",
            "Epoch 212/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9967\n",
            "Epoch 212: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0076 - acc: 0.9967 - val_loss: 1.8253 - val_acc: 0.7632\n",
            "Epoch 213/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0082 - acc: 0.9968\n",
            "Epoch 213: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0082 - acc: 0.9968 - val_loss: 1.7824 - val_acc: 0.7667\n",
            "Epoch 214/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9968\n",
            "Epoch 214: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 5s 9ms/step - loss: 0.0076 - acc: 0.9968 - val_loss: 1.8244 - val_acc: 0.7570\n",
            "Epoch 215/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9968\n",
            "Epoch 215: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0078 - acc: 0.9968 - val_loss: 1.7482 - val_acc: 0.7622\n",
            "Epoch 216/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9968\n",
            "Epoch 216: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0074 - acc: 0.9968 - val_loss: 1.8332 - val_acc: 0.7580\n",
            "Epoch 217/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9968\n",
            "Epoch 217: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0076 - acc: 0.9968 - val_loss: 1.7714 - val_acc: 0.7595\n",
            "Epoch 218/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9966\n",
            "Epoch 218: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0077 - acc: 0.9966 - val_loss: 1.7922 - val_acc: 0.7601\n",
            "Epoch 219/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9967\n",
            "Epoch 219: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0078 - acc: 0.9967 - val_loss: 1.7941 - val_acc: 0.7563\n",
            "Epoch 220/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9968\n",
            "Epoch 220: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0074 - acc: 0.9968 - val_loss: 1.7925 - val_acc: 0.7633\n",
            "Epoch 221/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0077 - acc: 0.9967\n",
            "Epoch 221: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0077 - acc: 0.9967 - val_loss: 1.7922 - val_acc: 0.7622\n",
            "Epoch 222/250\n",
            "479/479 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9967\n",
            "Epoch 222: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0076 - acc: 0.9967 - val_loss: 1.8426 - val_acc: 0.7655\n",
            "Epoch 223/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9967\n",
            "Epoch 223: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0079 - acc: 0.9967 - val_loss: 1.7761 - val_acc: 0.7612\n",
            "Epoch 224/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9968\n",
            "Epoch 224: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0075 - acc: 0.9968 - val_loss: 1.8540 - val_acc: 0.7576\n",
            "Epoch 225/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9968\n",
            "Epoch 225: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0076 - acc: 0.9968 - val_loss: 1.8247 - val_acc: 0.7588\n",
            "Epoch 226/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9968\n",
            "Epoch 226: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0077 - acc: 0.9968 - val_loss: 1.8129 - val_acc: 0.7599\n",
            "Epoch 227/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9967\n",
            "Epoch 227: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0081 - acc: 0.9968 - val_loss: 1.7948 - val_acc: 0.7591\n",
            "Epoch 228/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9968\n",
            "Epoch 228: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0075 - acc: 0.9968 - val_loss: 1.8154 - val_acc: 0.7593\n",
            "Epoch 229/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9968\n",
            "Epoch 229: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0075 - acc: 0.9968 - val_loss: 1.8019 - val_acc: 0.7594\n",
            "Epoch 230/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9969\n",
            "Epoch 230: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0075 - acc: 0.9969 - val_loss: 1.8617 - val_acc: 0.7576\n",
            "Epoch 231/250\n",
            "472/479 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9967\n",
            "Epoch 231: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0078 - acc: 0.9968 - val_loss: 1.8466 - val_acc: 0.7546\n",
            "Epoch 232/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9967\n",
            "Epoch 232: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0076 - acc: 0.9967 - val_loss: 1.8804 - val_acc: 0.7506\n",
            "Epoch 233/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9968\n",
            "Epoch 233: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0075 - acc: 0.9968 - val_loss: 1.8328 - val_acc: 0.7583\n",
            "Epoch 234/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9967\n",
            "Epoch 234: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0076 - acc: 0.9967 - val_loss: 1.8079 - val_acc: 0.7592\n",
            "Epoch 235/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9968\n",
            "Epoch 235: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0075 - acc: 0.9968 - val_loss: 1.8460 - val_acc: 0.7541\n",
            "Epoch 236/250\n",
            "477/479 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9967\n",
            "Epoch 236: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0077 - acc: 0.9967 - val_loss: 1.8688 - val_acc: 0.7593\n",
            "Epoch 237/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9968\n",
            "Epoch 237: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0076 - acc: 0.9968 - val_loss: 1.7986 - val_acc: 0.7653\n",
            "Epoch 238/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9969\n",
            "Epoch 238: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0074 - acc: 0.9969 - val_loss: 1.8470 - val_acc: 0.7555\n",
            "Epoch 239/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9967\n",
            "Epoch 239: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0074 - acc: 0.9968 - val_loss: 1.8419 - val_acc: 0.7537\n",
            "Epoch 240/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9967\n",
            "Epoch 240: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0074 - acc: 0.9967 - val_loss: 1.8335 - val_acc: 0.7573\n",
            "Epoch 241/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9967\n",
            "Epoch 241: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0081 - acc: 0.9967 - val_loss: 1.8510 - val_acc: 0.7563\n",
            "Epoch 242/250\n",
            "476/479 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9966\n",
            "Epoch 242: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0075 - acc: 0.9966 - val_loss: 1.8551 - val_acc: 0.7566\n",
            "Epoch 243/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9967\n",
            "Epoch 243: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0075 - acc: 0.9967 - val_loss: 1.7977 - val_acc: 0.7608\n",
            "Epoch 244/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9969\n",
            "Epoch 244: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0075 - acc: 0.9969 - val_loss: 1.8734 - val_acc: 0.7512\n",
            "Epoch 245/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9966\n",
            "Epoch 245: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0078 - acc: 0.9967 - val_loss: 1.8140 - val_acc: 0.7559\n",
            "Epoch 246/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9966\n",
            "Epoch 246: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0076 - acc: 0.9966 - val_loss: 1.7845 - val_acc: 0.7562\n",
            "Epoch 247/250\n",
            "478/479 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9969\n",
            "Epoch 247: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0074 - acc: 0.9969 - val_loss: 1.8052 - val_acc: 0.7560\n",
            "Epoch 248/250\n",
            "473/479 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9969\n",
            "Epoch 248: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0076 - acc: 0.9968 - val_loss: 1.8049 - val_acc: 0.7548\n",
            "Epoch 249/250\n",
            "474/479 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9969\n",
            "Epoch 249: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.0075 - acc: 0.9969 - val_loss: 1.8391 - val_acc: 0.7491\n",
            "Epoch 250/250\n",
            "475/479 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9967\n",
            "Epoch 250: val_acc did not improve from 0.81389\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.0076 - acc: 0.9968 - val_loss: 1.8110 - val_acc: 0.7546\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=250, callbacks=[mc], batch_size=256, validation_split=0.2)\n",
        "# batch size = 128, 256도 해볼 것"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9x6ljga5lJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e5f1fa-aa53-4600-f42b-7884eb4be53d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1708/1708 [==============================] - 6s 3ms/step - loss: 0.3648 - acc: 0.8377\n",
            "\n",
            " 테스트 정확도: 0.8377\n"
          ]
        }
      ],
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvWqHl_QgDT5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "93feca19-4679-4449-feeb-6a26c2013261"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 6 훈련 과정 시각화 (정확도)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ngdLoxggELb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "41143283-e76c-47b2-cefb-2b9bf7d92d8a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 7 훈련 과정 시각화 (손실)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZKXAM6D5lRX"
      },
      "outputs": [],
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "  # ㅋㅋ같은 그런 단어가 그렇게 긍정에 영향을 주는 거는 아닌 것 같음\n",
        "  clear_word = [\"ㅂ\",\"ㅈ\",\"ㄷ\",\"ㄱ\",\"ㅅ\",\"ㅛ\",\"ㅕ\",\"ㅑ\",\"ㅐ\",\"ㅔ\",\"ㅁ\",\"ㄴ\",\"ㅇ\",\"ㄹ\",\"ㅎ\",\"ㅗ\",\"ㅓ\",\"ㅏ\",\"ㅣ\",\"ㅋ\",\"ㅌ\",\"ㅊ\",\"ㅍ\",\"ㅠ\",\"ㅜ\",\"ㅡ\",\"!\",\"?\"]\n",
        "  for i in range(len(clear_word)):\n",
        "    new_sentence = new_sentence.replace(clear_word[i], \"\")\n",
        "\n",
        "  show_sentence = new_sentence\n",
        "  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
        "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
        "  #new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
        "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
        "  score = float(loaded_model.predict(pad_new)) # 예측\n",
        "  if(score > 0.5):\n",
        "    #print(\"{} : 긍정 리뷰입니다. {:.2f}점\".format(show_sentence,score))\n",
        "    #print(\"{:.2f}\".format(score))\n",
        "    return 1\n",
        "  #elif(score > 0.4):\n",
        "    #print(\"{} : 중립 리뷰입니다. {:.2f}점\".format(show_sentence,score))\n",
        "    #print(\"{:.2f}\".format(score))\n",
        "   # return 0\n",
        "  else:\n",
        "    #print(\"{} : 부정 리뷰입니다. {:.2f}점\".format(show_sentence,score))\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS7KhEgh5swh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88fadec8-45f3-4f73-a896-d162df37325c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sentence = [\"이재명과 문재인 가는 길이 다른이유22\",\"문재인 전 대통령 사저 평산마을 평화 위한 국민소송지원단 안내154\",\"문재인 지지자들과 이재명 지지자들을 스리슬적 갈라치기 할려는 조짐이 보이는데20\",\"저는 서울에 있어도 문재인 전 대통령 사저 양산마을에서 위법행위 저지르는 자들을 감76\",\"요즘 민주당 의원들 보니 문재인 대통령 욕했던 제가 미안할 지경입니다.14\",\"경기도지사는 원래 저쪽당껀데 이재명 도지사하고 문재인 대통령이 다져놓은거죠14\",\"문재인 전 대통령 사저 평산마을 위법행위자들 2차 소탕작전 6월10일 1일차 정리, 395\",\"신 전대협 의장 문재인, 역사에 문제人으로 남을 것14\",\"고 노무현 전 대통령님 13주기 - 문재인 & 이재명6\",\"문재인 전 대통령 사저 양산마을이 무법천지가 되고 마을주민들이 병원치료 받는 피해63\",\"문재인 전 대통령 사저 마을 어르신 10명, 집회 소음으로 병원 진료11\",\"문재인 대통령 사저 평산마을에서 난동 피운 안정권과 공범들 관련 소식49\",\"문재인 전 대통령 사저 평산마을에서 하루종일 온갖 욕설, 폭행, 협박, 업무방해 등 위111\",\"서민, 극빈층이 김대중, 노무현, 문재인을 폄하하는 이유는..15\",\"20대 때 노무현·30대 때 문재인…뼛속까지 파란 40대, 왜?20\",\"문재인의 팬덤과 이재명의 팬덤25\",\"이재명님은 이제 문재인 대통령이 걸어갔던 길 그대로 따라가시면 됩니다10\",\"문재인 대통령이 참 사람 보는 눈이 없긴 한가 봐요ㅠ57\",\"문재인 대통령님 고생많았습니다3\",\"문재인 사저 앞에서 확성기를 켜고 하는 시위도 괜찮다고요?7\",\"노무현 전 대통령 서거 13주기 추도식 후기feat. 문재인 대통령님, etc\",\"현근택 “문재인 정부는 실패했다...혁신가는 이재명뿐”23\",\"문재인당대표에게 안철수가 있었습니다1\",\"포스트 문재인이 없는게 아쉽네요7\",\"문재인 대통령 사저 평산마을 관련, 파렴치한 범죄자 안정권 엄벌탄원 및 구속촉구 진8\",\"문재인 정부 초대 총리\",\"민주당 문재인 당대표 전후에서 호남토호들 당흔들고 결국 뛰쳐나가던 꼴을 또 보겠어11\",\"제대로 경례하고도 욕 먹었던 문재인 대통령.JP955\",\"문재인 전 대통령 사저 평산마을 평화 위한 국민소송지원단 활동을 시작합니다86\",\"서점가에 퇴임한 `문재인 열풍`7\",\"홍준표, 마지막 유세서 문재인 맹공 후 MB 사면론..왜?7\",\"문재인정부 국정백서 발간 “위대한 국민과 함께 위기를 넘어 선진국으로”5\",\"오늘자 문재인 전 대통령 근황.페북38\",\"문재인 대통령이 당원에게 보낸 문자10\",\"한홍구 바이든-문재인 만남은 윤석열 정부에 대한 경고18\",\"특별편 : 문재인의 진심2\",\"굥이 취임이후 했던 일 고대로 문재인 대통령이 했다 칩시다?12\",\"문재인과 굳이 통화한 바이든, 미국의 메세지1\",\"청와대 본관에 걸린 문재인 대통령 초상화40\",\"문재인 전 대통령 딸, 사저 시위에 이게 과연 집회?‥입으로 총질9\",\"문재인 대통령이 2015년 당대표 1년 하면서 겪은 사건들3\",\"현재의 홍영표= 문재인 당대표시절 이종걸 포지션\",\"계양을 출마 이재명 패배 후 문재인 대통령께서 고생했다고 술 한 잔 주셨다7\",\"당원께 드리는 편지문재인.문자메시지7\",\"대선 결과나 현재 상황에 대해서 문재인 전 대통령 책임론 제기하는 것은12\",\"문재인 대통령 5년 화보집..눈물이 납니다.10\",\"문재인 대통령, 퇴임을 맞아 당원에게 드리는 편지\",\"찡찡이 GIF 문재인 대통령 다큐18\",\"오늘 문재인대통령 마지막 퇴근길 한줄평.zip10\",\"문재인 대통령님께 손편지 보냈는데 답장 받았습니다.52\",\"5년 만에 봉하마을 찾는 노무현의 친구 문재인3\",\"문재인정부 5년 화보집 외.pdf42\",\"문재인 대통령과 찡찡이 문재인정부 5년 다큐 특별편 : 문재인의 진심10\",\"자랑스러운 문재인 보유국\",\"제가 생각하는 문재인 정부의 최대 업적20\",\"문재인 정부 5년간의 소회3\",\"노무현 대통령 연설, 고생하셨습니다. 문재인 대통령님\",\"예전 지선때도 문재인대통령에게 선대위 맡겼었죠 아마1\",\"권영세 대북특사로 문재인 전 대통령.검토할 만하다24\",\"이재명 의원이 15 문재인 대표처럼 난관을 헤쳐나가리라 봅니다3\",\"문재인 정부 5년의 기록 1부 오직 평화입니다2\",\"노무현 전 대통령과 문재인 전 대통령 mbti.jpg5\",\"내일만 지나면 문재인 대통령이 퇴임하는게 아직 믿기지 않네요4\",\"펌 문프와 문재인 정부의 혜안 - 밀40\",\"왜 자꾸 문재인 정부 출산율을 들먹일까요21\",\"문재인 대통령: 임기 마지막 인사, 특별한 당부 말씀 | 문재인의 5년 대담31\",\"와~ 남편으로써 찐웃음 짓는 문재인 대통령.jpg24\",\"문재인 대통령, 5년의 사진 기록 공개29\",\"굥 대신에 문재인 전 대통령님이 봉하 오셨네요.1\",\"꿈에 문재인 전 대통령님 내외분이 나오셨습니다 -/4\",\"혐주의 대구서 붉은 페인트로 얼룩진 문재인 전 대통령 현수막 발견34\",\"문재인-바이든, 22일 서울에서 만난다.53\",\"전문, 영상문재인 대통령, 마지막 국무회의 모두 발언, 서면 브리핑33\",\"문재인 대통령, 직접 국민께 마지막 국민청원 답변11\",\"문재인 대통령과 맞붙었던 손수조 득표율12\",\"문재인과 민주당의 공통점8\",\"손실보상 문재인정부가 시스템 잘만들어놓은거 잘받아먹네요5\",\"외교정책 초보자인 윤 대통령이 문재인 전 대통령이 한 합의를 지키도록 하는 것”13\",\"문재인대통령님의 당원에게 드리는 편지1\",\"문재인 정부의 역사를 디지털로 아카이브 하려고 합니다.5\",\"문재인 대통령, 문재인 정부 마지막 날이네요7\",\"문재인 대통령에게 감사편지 보낸 결과?2\",\"문재인 대통령: 대한민국의 새로운 시대를 연 정부로 평가되고 기억되길 바란다. | 국9\",\"굥석열과 문재인대통령 사진 몇장 올려 봅니다.9\",\"문재인 대통령 사저 평산마을에서 불법행위 저지른 안정권 엄벌탄원 및 구속촉구139\",\"오늘 문재인 대통령님 퇴임날인데...4\",\"대통령의진심문재인정부 5년 특별판3\",\"제가 뽑은 성공한 대통령.. 문재인 대통령\",\"김어준 윤석열,... 문재인도 포토라인 세울 것12\",\"문재인 정부보다 낫다? 이걸 보고도 그런 말이 나오나\",\"갤럽 문재인 대통령 퇴임직전 지지율 및 굥 당선자 취임직전 지지율.jpg5\",\"문재인 안경테 샀습니다.31\",\"국민께 드리는 대통령의 마지막 편지 문재인정부 5년 다큐 특별편 : 문재인의 진심11\",\"문재인 정부 아파트 공급 허와 실178\",\"중국 시진핑주석도 방한하셔서 문재인 대통령과 만나셨으면 좋겠어요9\",\"대한민국 명예 대통령 문재인!\",\"김두일: 바이든이 문재인을 만나는 이유.jpg70\",\"나의 대통령 문재인. 깊이 감사 드립니다.\",\"📰단독문재인-바이든 만남 최종 무산..방한 전날 취소 통보24\",\"문재인 대통령님의 퇴임에 즈음하여 개인적인 생각2\",]\n",
        "print(len(sentence))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iToUDOuBXgD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a026c645-32fe-44bf-f969-6d072e83796b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "해당 사이트는 부정적 의견이 65개, 긍정적 의견이 35개로 현 정치인에 대해 부정적입니다.\n"
          ]
        }
      ],
      "source": [
        "what = 0  # sentiment_predict(sentence[i])이 무엇을 반환하는가?\n",
        "posi = 0  # 긍정 리뷰\n",
        "nega = 0  # 부정 리뷰\n",
        "neut = 0  # 중립 리뷰\n",
        "\n",
        "for i in range(len(sentence)):\n",
        "  what = sentiment_predict(sentence[i])\n",
        "  if what == -1: nega += 1\n",
        "  elif what == 0: neut += 1\n",
        "  elif what == 1: posi += 1\n",
        "\n",
        "if max(nega,posi) == nega: print(f\"해당 사이트는 부정적 의견이 {nega}개, 긍정적 의견이 {posi}개로 현 정치인에 대해 부정적입니다.\")\n",
        "elif max(nega,posi) == posi: print(f\"해당 사이트는 긍정적 의견이 {posi}개, 부정적 의견이 {nega}개로 현 정치인에 대해 긍정적입니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_cGQrdaWH8r"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "개별연구.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}